{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bac51e",
   "metadata": {},
   "source": [
    "# Part 0 - Data preparation\n",
    "\n",
    "In this notebook we will download the arXiv dataset and save it to S3. We will also do some light data preprocessing by only keeping the columns we need, filtering out reviews that are too short, and limiting the size of the datasets.\n",
    "\n",
    "To read more, please check out https://towardsdatascience.com/setting-up-a-text-summarisation-project-introduction-526622eea4a8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880efb0c",
   "metadata": {},
   "source": [
    "First of all we want to make sure that the relevent libraries are installed on this machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe74630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.81.1.tar.gz (519 kB)\n",
      "\u001b[K     |████████████████████████████████| 519 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets[s3]==1.6.2\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.63.1-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from transformers==4.6.1) (2.27.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp39-cp39-macosx_10_11_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: packaging in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from transformers==4.6.1) (21.3)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.3.15-cp39-cp39-macosx_10_9_x86_64.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-1.22.3-cp39-cp39-macosx_10_14_x86_64.whl (17.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.6 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=1.0.0<4.0.0\n",
      "  Downloading pyarrow-7.0.0-cp39-cp39-macosx_10_13_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.4.1-cp39-cp39-macosx_10_9_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-macosx_10_9_x86_64.whl (34 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3==1.16.43\n",
      "  Downloading boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.2.0-py3-none-any.whl (26 kB)\n",
      "Collecting botocore==1.19.52\n",
      "  Downloading botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.8)\n",
      "Collecting attrs==20.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.81.0.tar.gz (519 kB)\n",
      "\u001b[K     |████████████████████████████████| 519 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.80.0.tar.gz (517 kB)\n",
      "\u001b[K     |████████████████████████████████| 517 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.79.0.tar.gz (516 kB)\n",
      "\u001b[K     |████████████████████████████████| 516 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.78.0.tar.gz (513 kB)\n",
      "\u001b[K     |████████████████████████████████| 513 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.77.1.tar.gz (513 kB)\n",
      "\u001b[K     |████████████████████████████████| 513 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.77.0.tar.gz (513 kB)\n",
      "\u001b[K     |████████████████████████████████| 513 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.76.0.tar.gz (512 kB)\n",
      "\u001b[K     |████████████████████████████████| 512 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.75.1.tar.gz (511 kB)\n",
      "\u001b[K     |████████████████████████████████| 511 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from sagemaker>=2.48.0) (21.4.0)\n",
      "  Downloading sagemaker-2.75.0.tar.gz (511 kB)\n",
      "\u001b[K     |████████████████████████████████| 511 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.74.0.tar.gz (481 kB)\n",
      "\u001b[K     |████████████████████████████████| 481 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.73.0.tar.gz (481 kB)\n",
      "\u001b[K     |████████████████████████████████| 481 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.72.3.tar.gz (475 kB)\n",
      "\u001b[K     |████████████████████████████████| 475 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.72.2.tar.gz (473 kB)\n",
      "\u001b[K     |████████████████████████████████| 473 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.72.1.tar.gz (473 kB)\n",
      "\u001b[K     |████████████████████████████████| 473 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.72.0.tar.gz (477 kB)\n",
      "\u001b[K     |████████████████████████████████| 477 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.71.0.tar.gz (477 kB)\n",
      "\u001b[K     |████████████████████████████████| 477 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.70.0.tar.gz (466 kB)\n",
      "\u001b[K     |████████████████████████████████| 466 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.69.0.tar.gz (452 kB)\n",
      "\u001b[K     |████████████████████████████████| 452 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.1\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-macosx_10_9_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf3-to-dict>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "Collecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from sagemaker>=2.48.0) (4.8.2)\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.48.0) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from packaging->transformers==4.6.1) (3.0.4)\n",
      "Requirement already satisfied: six in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from requests->transformers==4.6.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from requests->transformers==4.6.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from requests->transformers==4.6.1) (2021.10.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from pandas->datasets[s3]==1.6.2) (2021.3)\n",
      "Collecting ppft>=1.6.6.4\n",
      "  Downloading ppft-1.6.6.4-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pox>=0.3.0\n",
      "  Downloading pox-0.3.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting aiohttp<=4\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-macosx_10_9_x86_64.whl (574 kB)\n",
      "\u001b[K     |████████████████████████████████| 574 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.2.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.0.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2021.11.1-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=2.0.1\n",
      "  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.11.0-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=1.4.1\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore~=1.4.1\n",
      "  Downloading aiobotocore-1.4.1.tar.gz (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.10.0-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.9.0-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore~=1.4.0\n",
      "  Downloading aiobotocore-1.4.0.tar.gz (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 781 kB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2021.8.0-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2021.7.0-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-2.2.0.tar.gz (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-2.0.0.tar.gz (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.0.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.14.0-cp39-cp39-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.10.0-py3-none-any.whl (23 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp39-cp39-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp39-cp39-macosx_10_9_x86_64.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl (28 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (4.1.1)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.1.0-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker, protobuf3-to-dict, aiobotocore\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.69.0-py2.py3-none-any.whl size=625709 sha256=c23b85816ee12a7d1a830bdf9b16f5cea57db6b67601be159a55c90acb68e99d\n",
      "  Stored in directory: /Users/robbdunlap/Library/Caches/pip/wheels/d7/8f/9d/2311179404dd882feb736a88de9cb56e4377445273ed4a7314\n",
      "  Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4030 sha256=e21ef053d83095c5183c664a1be9a1fdabda87312851ad3d9b50e9713fae6ffb\n",
      "  Stored in directory: /Users/robbdunlap/Library/Caches/pip/wheels/21/bf/76/90dd7b8d0598c7642532062ddff00ecef07df873c36396488c\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-1.2.2-py3-none-any.whl size=45750 sha256=f265cbae40fc79488047ff9b3aefcce89410cec6444509e79a281d9fcbfee25e\n",
      "  Stored in directory: /Users/robbdunlap/Library/Caches/pip/wheels/5c/8a/d9/8f981e444c8c2d2a0e1f9407fb8caa4d89530381be49a7042a\n",
      "Successfully built sagemaker protobuf3-to-dict aiobotocore\n",
      "Installing collected packages: multidict, frozenlist, yarl, jmespath, async-timeout, aiosignal, wrapt, tqdm, numpy, filelock, dill, botocore, aioitertools, aiohttp, xxhash, s3transfer, regex, pyarrow, protobuf, ppft, pox, pandas, multiprocess, joblib, huggingface-hub, fsspec, click, aiobotocore, tokenizers, smdebug-rulesconfig, sacremoses, s3fs, protobuf3-to-dict, pathos, google-pasta, datasets, boto3, transformers, sagemaker\n",
      "Successfully installed aiobotocore-1.2.2 aiohttp-3.8.1 aioitertools-0.10.0 aiosignal-1.2.0 async-timeout-4.0.2 boto3-1.16.43 botocore-1.19.52 click-8.1.0 datasets-1.6.2 dill-0.3.4 filelock-3.6.0 frozenlist-1.3.0 fsspec-2021.7.0 google-pasta-0.2.0 huggingface-hub-0.0.8 jmespath-0.10.0 joblib-1.1.0 multidict-6.0.2 multiprocess-0.70.12.2 numpy-1.22.3 pandas-1.4.1 pathos-0.2.8 pox-0.3.0 ppft-1.6.6.4 protobuf-3.19.4 protobuf3-to-dict-0.1.5 pyarrow-7.0.0 regex-2022.3.15 s3fs-2021.7.0 s3transfer-0.3.7 sacremoses-0.0.49 sagemaker-2.69.0 smdebug-rulesconfig-1.0.1 tokenizers-0.10.3 tqdm-4.49.0 transformers-4.6.1 wrapt-1.14.0 xxhash-3.0.0 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe89716a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from rouge_score) (1.22.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.0)\n",
      "Requirement already satisfied: joblib in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from nltk->rouge_score) (4.49.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from nltk->rouge_score) (2022.3.15)\n",
      "Installing collected packages: nltk, absl-py, rouge-score\n",
      "Successfully installed absl-py-1.0.0 nltk-3.7 rouge-score-0.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74713c30",
   "metadata": {},
   "source": [
    "We will download the dataset directly from the Kaggle website so we need to install the Kaggle Python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9749dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from kaggle) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from kaggle) (4.49.0)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-6.1.1-py2.py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: urllib3 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from kaggle) (1.26.8)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/robbdunlap/opt/anaconda3/envs/text_sum/lib/python3.9/site-packages (from requests->kaggle) (3.3)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=d8b3341c62e5979a445cd5b85752249f41737d679d52321875e840e72ff15b40\n",
      "  Stored in directory: /Users/robbdunlap/Library/Caches/pip/wheels/ac/b2/c3/fa4706d469b5879105991d1c8be9a3c2ef329ba9fe2ce5085e\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.5.12 python-slugify-6.1.1 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Kaggle/kaggle-api\n",
    "# follow the instruction in the above link to export Kaggle username and key to local environment so you don't \n",
    "# have to put it in the notebook (so it won't be exposed on GitHub)\n",
    "\n",
    "# only run the below if you aren't able to export these to your local environment through your shell\n",
    "# import os\n",
    "# os.environ['KAGGLE_USERNAME'] = \"<your-kaggle-username>\"\n",
    "# os.environ['KAGGLE_KEY'] = \"<your-kaggle-api-key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413c346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d7f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.api.dataset_download_files('Cornell-University/arxiv', path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2b814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  arxiv.zip\n",
      "  inflating: arxiv-metadata-oai-snapshot.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip arxiv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd20d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beba5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir raw_data\n",
    "!mv arxiv.zip raw_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8100b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv arxiv-metadata-oai-snapshot.json raw_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc9f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-data_dir=.%2Fraw_data%2F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset arxiv_dataset/default (download: Unknown size, generated: 2.09 GiB, post-processed: Unknown size, total: 2.09 GiB) to /Users/robbdunlap/.cache/huggingface/datasets/arxiv_dataset/default-data_dir=.%2Fraw_data%2F/1.1.0/242eb95c95350194872f5be3fb00e7938e53b0944442e85f45a5d2240328f370...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset arxiv_dataset downloaded and prepared to /Users/robbdunlap/.cache/huggingface/datasets/arxiv_dataset/default-data_dir=.%2Fraw_data%2F/1.1.0/242eb95c95350194872f5be3fb00e7938e53b0944442e85f45a5d2240328f370. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"arxiv_dataset\", data_dir='./raw_data/', split='train', ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605aae99",
   "metadata": {},
   "source": [
    "The original dataset is too long, so we shuffle it and limit the number of records to 25,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6efedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'update_date'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.select(range(25000))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c0f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d41258",
   "metadata": {},
   "outputs": [],
   "source": [
    " # only keep columns that are required\n",
    "df = df[['abstract', 'title']]\n",
    "df = df.rename(columns={\"abstract\": \"text\", \"title\": \"summary\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45044aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657c2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0149280b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The rest-frame UV spectra of three recent tidal disruption events (TDEs), ASASSN-14li, PTF15af...</td>\n",
       "      <td>Carbon and Nitrogen Abundance Ratio In the Broad Line Region of Tidal   Disruption Events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inspired by the success of transformer-based pre-training methods on natural language tasks an...</td>\n",
       "      <td>Survey: Transformer based Video-Language Pre-training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pandharipande-Pixton have used the geometry of the moduli space of stable quotients to produce...</td>\n",
       "      <td>Tautological relations in moduli spaces of weighted pointed curves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suppose X is a projective toric scheme defined over a commutative ring R equipped with an ampl...</td>\n",
       "      <td>A splitting result for the algebraic K-theory of projective toric   schemes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We introduce an approach that accurately reconstructs 3D human poses and detailed 3D full-body...</td>\n",
       "      <td>Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies   from Single RGB Images</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0    The rest-frame UV spectra of three recent tidal disruption events (TDEs), ASASSN-14li, PTF15af...   \n",
       "1    Inspired by the success of transformer-based pre-training methods on natural language tasks an...   \n",
       "2    Pandharipande-Pixton have used the geometry of the moduli space of stable quotients to produce...   \n",
       "3    Suppose X is a projective toric scheme defined over a commutative ring R equipped with an ampl...   \n",
       "4    We introduce an approach that accurately reconstructs 3D human poses and detailed 3D full-body...   \n",
       "\n",
       "                                                                                          summary  \n",
       "0       Carbon and Nitrogen Abundance Ratio In the Broad Line Region of Tidal   Disruption Events  \n",
       "1                                           Survey: Transformer based Video-Language Pre-training  \n",
       "2                              Tautological relations in moduli spaces of weighted pointed curves  \n",
       "3                     A splitting result for the algebraic K-theory of projective toric   schemes  \n",
       "4  Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies   from Single RGB Images  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c4750",
   "metadata": {},
   "source": [
    "## Filtering the dataset\n",
    "\n",
    "We want to discard reviews and titles that are too short, so that our model can produce more interesting summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53102dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_summary = 5\n",
    "cutoff_text = 20\n",
    "df = df[(df['summary'].apply(lambda x: len(x.split()) >= cutoff_summary)) & (df['text'].apply(lambda x: len(x.split()) >= cutoff_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769ad53",
   "metadata": {},
   "source": [
    "## Limiting the size of the datasets and splitting\n",
    "\n",
    "We want to limit the size of the datasets so that training of the model can finish in a reasonable amount of time. This is a decision that we might want to revisit in the experimentation phase if we want to increase the performance of the model. We then split the dataset into test (80%), validation (10%), and test (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f42ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(20000, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# split the dataset into train, val, and test\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=44), [int(0.8*len(df)), int((0.9)*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c36f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data/train.csv', index=False)\n",
    "df_val.to_csv('data/val.csv', index=False)\n",
    "df_test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98223a6c",
   "metadata": {},
   "source": [
    "## Save the data as CSV files and upload them to S3\n",
    "\n",
    "We need to upload the data to S3 in order to train the model at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c83b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccac070",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp data/train.csv s3://$bucket/summarization/data/train.csv\n",
    "!aws s3 cp data/val.csv s3://$bucket/summarization/data/val.csv\n",
    "!aws s3 cp data/test.csv s3://$bucket/summarization/data/test.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
